
# ðŸ“Œ MERN Project â€” Local Chat App using Groq SDK (Minor Project)
    This is a minor MERN project that demonstrates how to build a simple AI Chat Application using:
    Express.js
    React.js
    Node.js
    Groq SDK (LLAMA 3.1 Models)

    The project allows users to send messages from a React frontend, post them to a backend /chat endpoint, and receive AI-generated replies.

# ðŸš€ Features

    âœ”  React Chat UI Similar to chatgpt
    âœ” Backend /chat API using Groq SDK
    âœ” Sends user prompt â†’ Receives AI response
    âœ” End-to-end working MERN pipeline
    âœ” Easy to extend (authentication, DB storage, chat history, etc.)

#    | Layer    | Technology                          |
    | -------- | ----------------------------------- |
    | Frontend | React + Axios                       |
    | Backend  | Node.js, Express.js                 |
    | AI Model | Groq SDK (LLaMA 3.1)                |


#    project/
    â”œâ”€â”€ backend/
    â”‚    â””â”€â”€ index.js
    |    â””â”€â”€ store.js

    â”œâ”€â”€ frontend/
    â”‚    â””â”€â”€ src/
    â”‚         â””â”€â”€ App.jsx
    â””â”€â”€ README.md

    git clone https://github.com/Shamsherkhan7549/GENTAI-NODE/tree/main/day-7-LLM

#   ðŸ§  Groq SDK Usage Example
    const response = await client.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [{ role: "user", content: message }]
    });

# ðŸŽ¯ What You Learned
    How to send user input from React
    How to call LLM models using Groq SDK
    How to build a simple MERN AI application
    How to create a functional end-to-end chat demo

#   ðŸ“Œ Future Improvements
        Store chat history in MongoDB
        Add login/signup
        Add streaming responses
        Add dark mode UI
        Deploy using Render / Vercel



